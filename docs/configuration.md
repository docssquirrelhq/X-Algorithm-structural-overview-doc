---
title: Configuration and Dependencies
slug: configuration
description: Overview of project dependencies, configuration dataclasses, runtime parameters, and initialization handling in the Phoenix recommendation system.
sidebar_label: Configuration
sidebar_position: 12
---

# Configuration and Dependencies

This page covers the configuration and dependency management in the Phoenix recommendation system codebase. The project is primarily Python-based, leveraging JAX and Haiku for neural network modeling. We discuss the Python dependency files (`pyproject.toml` and `uv.lock`), the absence of Rust equivalents, runtime parameters (e.g., embedding sizes, hash configurations, sequence lengths), and model initialization with dtype handling (e.g., bfloat16).

For an overview of the Python components, see [Python Phoenix Overview](../python-phoenix-overview.md). Model details are in [RecSys Model](../recsys-model.md) and [RecSys Retrieval Model](../recsys-retrieval-model.md).

## Python Dependencies

The project uses modern Python packaging standards with `pyproject.toml` for declarative configuration and `uv.lock` for reproducible dependency resolution. The project is named "grok-1" (version 0.1.0) and requires Python >=3.11. It targets macOS (Darwin) and Linux platforms, with linting configured via Ruff (e.g., indent width 4, line length 100, ignoring specific errors like E722 for lambda assignments).

### pyproject.toml

This file defines the core project metadata and dependencies:

- **Runtime Dependencies**:
  - `dm-haiku >=0.0.13`: JAX-based library for building neural networks (Haiku).
  - `jax ==0.8.1`: High-performance numerical computing and automatic differentiation library.
  - `numpy >=1.26.4`: Array operations and mathematical functions.
  - `pyright >=1.1.408`: Static type checking tool.

- **Development Dependencies** (under `[dependency-groups].dev`):
  - `pytest`: Testing framework (version resolved in lockfile).

No build system (e.g., setuptools) is explicitly configured beyond `uv` usage. The file references `README.md` for further documentation.

### uv.lock

Generated by the `uv` package manager, this lockfile ensures reproducible installations by pinning exact versions, sources (PyPI), hashes, and platform-specific wheels/sdists. It supports Python 3.11–3.14 (CPython and PyPy) on macOS and Linux (x86_64, aarch64). The lock revision is 3, prioritizing wheels for faster installs.

Key locked packages (focusing on JAX, Haiku, pytest):

- **dm-haiku (0.0.16)**: Depends on `absl-py (2.3.1)`, `jmp (0.0.4)`, `numpy`, `tabulate (0.9.0)`. Wheels ~374 KB.
- **jax (0.8.1)**: Depends on `jaxlib (0.8.1)` (platform-specific, ~55–80 MB), `ml-dtypes (0.5.4)` (supports bfloat16), `numpy`, `opt-einsum (3.4.0)`, `scipy (1.17.0)`. Wheels ~2.9 MB.
- **pytest (9.0.2, dev-only)**: Depends on `iniconfig (2.3.0)`, `packaging (25.0)`, `pluggy (1.6.0)`, `pygments`. Wheel ~375 KB.
- **numpy (2.4.1)**: Core array support, wheels ~12–18 MB across platforms.
- **pyright (1.1.408)**: Type checker, wheel ~6.4 MB.

Other supporting packages include `typing-extensions (4.15.0)`. The full graph has ~20 packages with SHA256 hashes for security. Install via `uv sync` for reproducibility.

:::tip Installation Tip
Run `uv sync --dev` in the project root to install all dependencies, including dev tools like pytest. This uses the lockfile for exact versions.
:::

## Rust Equivalents (Cargo.toml)

The codebase is exclusively Python-focused, with no Rust components or `Cargo.toml` files present. There are no mixed-language bindings (e.g., via PyO3 or maturin). All neural network logic, including transformers and embeddings, is implemented in Python using JAX and Haiku. For Rust-based systems, equivalents would include `Cargo.toml` for crate dependencies and `Cargo.lock` for locking, but these are not applicable here.

## Runtime Parameters

Runtime configuration is handled via dataclasses in `recsys_model.py` and `recsys_retrieval_model.py`. These define model hyperparameters like embedding sizes, sequence lengths, and hash settings. Defaults are provided where possible, but many (e.g., `emb_size`) must be set explicitly in scripts like `run_retrieval.py` or `run_ranker.py`.

### PhoenixModelConfig (Ranking Model)

Configures the core recommendation ranking model:

```python
@dataclass
class PhoenixModelConfig:
    model: TransformerConfig
    emb_size: int  # Embedding dimension (e.g., 128)
    num_actions: int  # Number of engagement actions (e.g., 19)
    history_seq_len: int = 128  # Max history sequence length
    candidate_seq_len: int = 32  # Max candidate sequence length
    name: Optional[str] = None
    fprop_dtype: Any = jnp.bfloat16  # Forward prop dtype
    hash_config: HashConfig = None
    product_surface_vocab_size: int = 16  # Vocab size for product surfaces
```

- **Key Parameters**:
  - `emb_size`: Dimension of embeddings (no default; runtime example: 128). Used for projections in user/item representations.
  - `history_seq_len`: Length of user history sequences (default: 128; runtime override: 32).
  - `candidate_seq_len`: Length of candidate sequences (default: 32; runtime override: 8).
  - `hash_config`: Defaults to `HashConfig()` if None (see below).
  - `num_actions`: Derived from engagement actions (e.g., "favorite_score", "reply_score"; total 19).

### HashConfig

Handles hash-based embeddings for sparse features:

```python
@dataclass
class HashConfig:
    num_user_hashes: int = 2  # User hash functions
    num_item_hashes: int = 2  # Item (post) hash functions
    num_author_hashes: int = 2  # Author hash functions
```

- Used in batch creation and embedding reductions (e.g., combining hashes into `[B, num_user_hashes * D]`).

### PhoenixRetrievalModelConfig (Retrieval Model)

Similar to `PhoenixModelConfig` but for two-tower retrieval:

- Shares parameters: `emb_size`, `history_seq_len=128`, `candidate_seq_len=32`, `fprop_dtype=jnp.bfloat16`, `hash_config=HashConfig()`.
- No `num_actions` (retrieval-focused).

### TransformerConfig (Shared Base)

Underlies both models:

```python
@dataclass
class TransformerConfig:
    emb_size: int  # Embedding dimension
    key_size: int  # Attention key dimension (e.g., 64)
    num_q_heads: int  # Query heads (e.g., 2)
    num_kv_heads: int  # Key-value heads (e.g., 2)
    num_layers: int  # Transformer layers (e.g., 2)
    widening_factor: float = 4.0  # FFN widening (runtime: 2.0)
    attn_output_multiplier: float = 1.0  # Attention scaling (runtime: 0.125)
```

- FFN size computed as `int(widening_factor * emb_size) * 2 // 3`, padded to multiples of 8.

Runtime scripts (e.g., `run_retrieval.py`) override defaults for demos:

- `emb_size=128`, `history_seq_len=32`, `candidate_seq_len=8`.
- `hash_config` explicitly set to 2 hashes each.
- Batch size: `bs_per_device=0.125`, scaled by GPU count (e.g., `batch_size = max(1, int(bs_per_device * num_local_gpus))`).
- Seed: `rng_seed=42` for PRNG initialization.

For testing (e.g., `test_recsys_retrieval_model.py`), smaller values like `emb_size=64`, `history_seq_len=16` are used.

## Initialization and Dtype Handling

Models are initialized lazily via `make()` methods, which call `initialize()` to set flags and instantiate components. Dtypes default to `jnp.bfloat16` for efficient forward passes (via `ml-dtypes`), with fallbacks to `jnp.float32` for parameters and computations.

### Initialization Flow

1. **Config Setup**: Create dataclass (e.g., `PhoenixModelConfig(...)`), auto-initializing `hash_config`.
2. **Model Creation**: Call `config.make()` to build `PhoenixModel` or `PhoenixRetrievalModel` with transformer and dtype.
3. **Runner Initialization** (e.g., in `ModelRunner` or `RetrievalModelRunner`):
   - Set `model.fprop_dtype = jnp.bfloat16`.
   - Use Haiku: `hk.transform(forward)` for pure functions.
   - Initialize params: `params = forward.init(rng, dummy_data, ...)` with `jax.random.PRNGKey(rng_seed)`.
   - Dummy batches created via `create_dummy_batch_from_config` (uses config params like sequence lengths, hashes).
   - Embeddings: `create_dummy_embeddings_from_config` (np.float32, cast to `fprop_dtype`).

Example from `run_ranker.py`:

```python
recsys_model = PhoenixModelConfig(
    emb_size=128,
    num_actions=19,
    history_seq_len=32,
    candidate_seq_len=8,
    hash_config=HashConfig(num_user_hashes=2, ...),
    model=TransformerConfig(emb_size=128, widening_factor=2, ...),
)

inference_runner = RecsysInferenceRunner(
    runner=ModelRunner(model=recsys_model, bs_per_device=0.125),
    name="recsys_local",
)
inference_runner.initialize()  # Loads dummy data, transforms forward fn
```

### Dtype Details

- **Forward Propagation**: `fprop_dtype=jnp.bfloat16` for embeddings and outputs (e.g., `action_emb.astype(self.fprop_dtype)`).
- **Computations**: Cast to `jnp.float32` in layers (e.g., RMS norm: input to float32, output to `fprop_dtype`).
- **Attention Masks**: Default `jnp.float32`.
- **Initialization**: Variance scaling (`hk.initializers.VarianceScaling(1.0, "fan_out")`) for embeddings/projections.
- **Inference**: Uses `hk.without_apply_rng` to apply params without RNG.

This setup ensures efficient JAX computations on GPUs/TPUs, with bfloat16 reducing memory usage while maintaining precision.

:::caution Performance Note
bfloat16 requires compatible hardware (e.g., NVIDIA Ampere+ or Apple Silicon). Fallback to float32 if needed by overriding `fprop_dtype`.
:::

For runners and data flow, see [Runners](../runners.md) and [Data Flow](../data-flow.md).